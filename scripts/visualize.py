"""
visualize.py: Enhanced version for Threshold and Runtime analysis.
Usage: python visualize.py
"""
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np


# Load the data generated by scoring.py
# (Assuming results.csv contains: true_runtime, pred_runtime, true_thresh, pred_thresh, etc.)
df = pd.read_csv("results.csv")


# Setup Plot - 2x2 grid for Thresholds and Runtimes
fig, axes = plt.subplots(2, 2, figsize=(16, 12))


# --- PLOT 1: The Threshold "Kill Zone" ---
ladder = [1, 2, 4, 8, 16, 32, 64, 128, 256]
plot_df = df[df['true_thresh'].isin(ladder) & df['pred_thresh'].isin(ladder)]
confusion = pd.crosstab(
   plot_df['true_thresh'],
   plot_df['pred_thresh']
).reindex(index=ladder, columns=ladder, fill_value=0)


sns.heatmap(confusion, annot=True, fmt='d', cmap="Reds", cbar=False, ax=axes[0, 0])
axes[0, 0].set_title("Threshold Confusion Matrix\n(Bottom-Left = SCORE 0 [FATAL])")
axes[0, 0].set_ylabel("True Threshold")
axes[0, 0].set_xlabel("Predicted Threshold")
axes[0, 0].plot([0, 9], [0, 9], 'g--', linewidth=3) # Perfect line


# --- PLOT 2: Runtime Regression (Log-Log Scale) ---
# Predicting log(runtime) is the suggested modeling tip
sns.scatterplot(x='true_time', y='pred_time', data=df, alpha=0.6, ax=axes[0, 1])
axes[0, 1].set_xscale('log')
axes[0, 1].set_yscale('log')
# Add 1-to-1 diagonal line
max_val = max(df['true_time'].max(), df['pred_time'].max())
min_val = min(df['true_time'].min(), df['pred_time'].min())
axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)
axes[0, 1].set_title("Runtime: Predicted vs True (Log-Log Scale)")
axes[0, 1].set_xlabel("True Runtime (s)")
axes[0, 1].set_ylabel("Predicted Runtime (s)")


# --- PLOT 3: Error Breakdown ---
sns.countplot(y='error_type', data=df, order=df['error_type'].value_counts().index, ax=axes[1, 0], palette="viridis")
axes[1, 0].set_title("Error Type Distribution")


# --- PLOT 4: Score Distribution ---
# The score reflects how well you balanced the threshold constraint and runtime accuracy.
sns.histplot(df['time_score'], bins=20, kde=False, ax=axes[1, 1], color='gold', edgecolor='black')
axes[1, 1].set_title("Distribution of Task Runtime score\n(Higher is Better)")
axes[1, 1].set_xlabel("Runtimes score per Task")
axes[1, 1].set_ylabel("Frequency (Number of Tasks)")


# Add a vertical line for the average score to track performance
mean_score = df['time_score'].mean()
axes[1, 1].axvline(mean_score, color='red', linestyle='--', label=f'Avg: {mean_score:.2f}')
axes[1, 1].legend()


plt.tight_layout()
plt.show()


# --- PRINT TEXT REPORT ---
print("--- SUMMARY STATISTICS ---")
mae = np.mean(np.abs(df['true_time'] - df['pred_time']))
print(f"Mean Absolute Error (Runtime): {mae:.4f} seconds")
print(f"Total Fatal Errors (Score 0): {len(df[df['score'] == 0])}")